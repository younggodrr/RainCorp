"""
MagnaAgent Core - ReAct Pattern Implementation

This module implements the central orchestration component for the Magna AI Agent
using the ReAct (Reason + Act) pattern for structured decision-making.

The ReAct cycle follows: Analyze → Plan → Act → Respond

**Validates: Requirements 7.1-7.7**
"""

import json
import logging
import asyncio
from dataclasses import dataclass
from datetime import datetime
from typing import Any, AsyncIterator, Dict, List, Optional
from uuid import uuid4

from ..llm.orchestrator import LLMOrchestrator
from ..memory.system import MemorySystem
from ..tools.base import ToolRegistry, ToolResult
from ..matching.opportunity import OpportunityMatcher
from ..matching.collaboration import CollaborationMatcher
from ..interview.preparation import InterviewPreparationModule
from ..documents.manager import DocumentManager
from ..documents.consent import ConsentManager
from ..utils.performance import ProgressIndicator, ProgressUpdate, RequestCache, ParallelExecutor
from .prompts import (
    BASE_SYSTEM_PROMPT,
    build_system_prompt,
    get_analysis_prompt,
    get_planning_prompt,
)

logger = logging.getLogger(__name__)


@dataclass
class Context:
    """Context for agent processing."""
    user_id: str
    conversation_id: str
    message: str
    memory_entries: List[Any]
    metadata: Dict[str, Any]


@dataclass
class Analysis:
    """Result of the Analyze phase."""
    intent: str  # User's primary intent
    required_information: List[str]  # What info is needed
    entities: Dict[str, Any]  # Extracted entities
    confidence: float  # Confidence in analysis (0-1)


@dataclass
class ActionPlan:
    """Result of the Plan phase."""
    tools_to_use: List[str]  # Tool names in execution order
    tool_parameters: Dict[str, Dict[str, Any]]  # Parameters for each tool
    execution_strategy: str  # "sequential" or "parallel"
    reasoning: str  # Why this plan was chosen


@dataclass
class ActionResults:
    """Result of the Act phase."""
    tool_results: Dict[str, ToolResult]  # Tool name -> result
    success: bool  # Overall success
    errors: List[str]  # Any errors encountered


@dataclass
class AgentResponse:
    """Response from agent processing."""
    content: str  # Response text
    conversation_id: str
    metadata: Dict[str, Any]  # Tool calls, results, etc.
    timestamp: datetime


class MagnaAgent:
    """
    Central orchestration component implementing the ReAct pattern.
    
    The agent processes user messages through a structured cycle:
    1. ANALYZE: Parse intent and identify required information
    2. PLAN: Select tools and determine execution order
    3. ACT: Execute tool calls with error handling
    4. RESPOND: Generate final response using LLM
    
    The agent integrates with:
    - LLM Orchestrator for response generation
    - Memory System for context retrieval
    - Tool Registry for action execution
    - Matching engines for opportunity/collaboration recommendations
    - Interview preparation for career guidance
    - Document management for file handling
    - Consent management for user approval
    
    **Validates: Requirements 7.1-7.7**
    """
    
    # System prompt defining agent persona and behavior
    # Note: This is kept for backward compatibility. Use prompts.py for new implementations.
    SYSTEM_PROMPT = BASE_SYSTEM_PROMPT

    def __init__(
        self,
        llm_orchestrator: LLMOrchestrator,
        memory_system: MemorySystem,
        tool_registry: ToolRegistry,
        opportunity_matcher: Optional[OpportunityMatcher] = None,
        collaboration_matcher: Optional[CollaborationMatcher] = None,
        interview_module: Optional[InterviewPreparationModule] = None,
        document_manager: Optional[DocumentManager] = None,
        consent_manager: Optional[ConsentManager] = None,
        system_prompt: Optional[str] = None,
        progress_threshold_seconds: float = 2.0
    ):
        """
        Initialize MagnaAgent with dependencies.
        
        Args:
            llm_orchestrator: LLM orchestrator for response generation
            memory_system: Memory system for context retrieval
            tool_registry: Tool registry for action execution
            opportunity_matcher: Optional opportunity matching engine
            collaboration_matcher: Optional collaboration matching engine
            interview_module: Optional interview preparation module
            document_manager: Optional document management system
            consent_manager: Optional consent management system
            system_prompt: Optional custom system prompt (uses default if None)
            progress_threshold_seconds: Time threshold for progress indicators (default: 2.0s)
        """
        self.llm_orchestrator = llm_orchestrator
        self.memory_system = memory_system
        self.tool_registry = tool_registry
        self.opportunity_matcher = opportunity_matcher
        self.collaboration_matcher = collaboration_matcher
        self.interview_module = interview_module
        self.document_manager = document_manager
        self.consent_manager = consent_manager
        self.system_prompt = system_prompt or self.SYSTEM_PROMPT
        self.progress_indicator = ProgressIndicator(threshold_seconds=progress_threshold_seconds)
        self.request_cache = RequestCache(default_ttl_seconds=300)  # 5 minute cache
        
        logger.info("MagnaAgent initialized with ReAct pattern")
    
    async def process_message(
        self,
        user_id: str,
        message: str,
        conversation_id: str,
        stream: bool = True
    ) -> AsyncIterator[AgentResponse]:
        """
        Process user message through ReAct cycle.
        
        This is the main entry point for agent processing. It orchestrates
        the complete ReAct cycle: Analyze → Plan → Act → Respond.
        
        Shows progress indicators when processing takes longer than threshold.
        Checks cache for frequently asked questions before processing.
        
        Args:
            user_id: User ID
            message: User's input message
            conversation_id: Conversation ID for context
            stream: Whether to stream response chunks
            
        Yields:
            Progress updates (if slow), then AgentResponse chunks if streaming,
            or complete response
            
        **Validates: Requirements 7.1, 10.2, 10.3**
        """
        request_id = str(uuid4())
        start_time = datetime.now()
        
        logger.info(
            f"Processing message: request_id={request_id}, user={user_id}, "
            f"conversation={conversation_id}, stream={stream}"
        )
        
        # Check cache first
        cache_context = {"user_id": user_id}
        cached_response = self.request_cache.get(message, cache_context)
        
        if cached_response is not None:
            logger.info(f"[{request_id}] Cache hit for query")
            # Return cached response
            cached_response.metadata["request_id"] = request_id
            cached_response.metadata["from_cache"] = True
            cached_response.timestamp = datetime.now()
            yield cached_response
            return
        
        logger.debug(f"[{request_id}] Cache miss, processing query")
        
        async def _process_internal():
            """Internal processing function for progress tracking."""
            try:
                # Retrieve memory context
                memory_entries = await self.memory_system.retrieve_context(
                    user_id=user_id,
                    conversation_id=conversation_id,
                    query=message,
                    max_results=5
                )
                
                # Build context
                context = Context(
                    user_id=user_id,
                    conversation_id=conversation_id,
                    message=message,
                    memory_entries=memory_entries,
                    metadata={
                        "request_id": request_id,
                        "start_time": start_time.isoformat()
                    }
                )
                
                # FAST PATH: Check if this is a simple conversational query
                # Skip Analyze and Plan phases for better performance
                is_simple_query = self._is_simple_conversational_query(message)
                
                if is_simple_query:
                    logger.debug(f"[{request_id}] Using fast path for simple query")
                    
                    # Skip directly to RESPOND phase
                    response_chunks = []
                    async for response_chunk in self._respond_simple(
                        message=message,
                        context=context,
                        stream=stream
                    ):
                        response_chunks.append(response_chunk)
                    
                    execution_time = (datetime.now() - start_time).total_seconds()
                    logger.info(
                        f"[{request_id}] Message processing complete (fast path) in {execution_time:.2f}s"
                    )
                    
                    # Cache simple responses
                    if len(response_chunks) > 0:
                        self.request_cache.set(
                            query=message,
                            response=response_chunks[0],
                            context=cache_context,
                            ttl_seconds=300
                        )
                    
                    return response_chunks
                
                # FULL ReAct PATH: For complex queries requiring tools
                # PHASE 1: ANALYZE
                logger.debug(f"[{request_id}] Starting ANALYZE phase")
                analysis = await self._analyze(message, context)
                logger.info(
                    f"[{request_id}] Analysis complete: intent={analysis.intent}, "
                    f"confidence={analysis.confidence:.2f}"
                )
                
                # PHASE 2: PLAN
                logger.debug(f"[{request_id}] Starting PLAN phase")
                plan = await self._plan(analysis, context)
                logger.info(
                    f"[{request_id}] Plan complete: tools={plan.tools_to_use}, "
                    f"strategy={plan.execution_strategy}"
                )
                
                # PHASE 3: ACT
                logger.debug(f"[{request_id}] Starting ACT phase")
                results = await self._act(plan, context)
                logger.info(
                    f"[{request_id}] Action complete: success={results.success}, "
                    f"tools_executed={len(results.tool_results)}"
                )
                
                # PHASE 4: RESPOND
                logger.debug(f"[{request_id}] Starting RESPOND phase")
                response_chunks = []
                async for response_chunk in self._respond(
                    analysis=analysis,
                    plan=plan,
                    results=results,
                    context=context,
                    stream=stream
                ):
                    response_chunks.append(response_chunk)
                
                # Store interaction in memory
                logger.debug(f"[{request_id}] Storing interaction in memory")
                
                execution_time = (datetime.now() - start_time).total_seconds()
                logger.info(
                    f"[{request_id}] Message processing complete in {execution_time:.2f}s"
                )
                
                # Cache the response if it's cacheable (no tool calls or simple queries)
                if len(response_chunks) > 0 and not plan.tools_to_use:
                    logger.debug(f"[{request_id}] Caching response")
                    # Cache the first/main response
                    self.request_cache.set(
                        query=message,
                        response=response_chunks[0],
                        context=cache_context,
                        ttl_seconds=300  # 5 minutes
                    )
                
                return response_chunks
                
            except Exception as e:
                logger.error(
                    f"[{request_id}] Error processing message: {e}",
                    exc_info=True
                )
                
                # Generate error response
                error_response = AgentResponse(
                    content=self._generate_error_message(e),
                    conversation_id=conversation_id,
                    metadata={
                        "request_id": request_id,
                        "error": str(e),
                        "error_type": type(e).__name__
                    },
                    timestamp=datetime.now()
                )
                
                return [error_response]
        
        # Track request with progress indicator
        async for item in self.progress_indicator.track_request(
            request_id=request_id,
            operation=_process_internal
        ):
            # Check if it's a progress update or actual response
            if isinstance(item, ProgressUpdate):
                # Yield progress update as a special response
                progress_response = AgentResponse(
                    content="",
                    conversation_id=conversation_id,
                    metadata={
                        "request_id": request_id,
                        "progress": item.to_dict(),
                        "is_progress_update": True
                    },
                    timestamp=item.timestamp
                )
                yield progress_response
            else:
                # Yield actual response chunks
                for response in item:
                    yield response
    
    async def _analyze(self, message: str, context: Context) -> Analysis:
        """
        Analyze user intent and identify required information.
        
        This phase parses the user's message to understand:
        - What they want to accomplish (intent)
        - What information is needed to fulfill the request
        - Key entities mentioned (skills, locations, roles, etc.)
        - Confidence in the analysis
        
        Args:
            message: User's input message
            context: Processing context with memory
            
        Returns:
            Analysis with intent, required info, and entities
            
        **Validates: Requirements 7.2**
        """
        logger.debug(f"Analyzing message: {message[:100]}...")
        
        # Build analysis prompt with memory context
        memory_context = self._format_memory_context(context.memory_entries)
        
        analysis_prompt = f"""Analyze the following user message and determine their intent.

User message: "{message}"

Recent conversation context:
{memory_context}

Identify:
1. Primary intent (what does the user want?)
2. Required information (what data/tools are needed?)
3. Key entities (skills, locations, roles, companies, etc.)
4. Confidence level (0.0-1.0)

Respond in JSON format:
{{
  "intent": "brief description of user's goal",
  "required_information": ["info1", "info2"],
  "entities": {{"entity_type": "value"}},
  "confidence": 0.85
}}

Common intents:
- find_opportunities: User wants job/project/gig recommendations
- find_collaborators: User wants to find team members
- interview_prep: User wants interview practice or questions
- document_help: User wants to upload/submit documents
- career_advice: User wants general career guidance
- clarification_needed: User's request is unclear"""

        # Generate analysis using LLM
        response_chunks = []
        async for chunk in self.llm_orchestrator.generate(
            prompt=analysis_prompt,
            system_prompt=get_analysis_prompt(),
            temperature=0.3,  # Lower temperature for more consistent analysis
            max_tokens=512
        ):
            response_chunks.append(chunk)
        
        response_text = "".join(response_chunks)
        
        # Parse JSON response
        analysis_data = self._parse_json_response(response_text)
        
        return Analysis(
            intent=analysis_data.get("intent", "unknown"),
            required_information=analysis_data.get("required_information", []),
            entities=analysis_data.get("entities", {}),
            confidence=float(analysis_data.get("confidence", 0.5))
        )
    
    async def _plan(self, analysis: Analysis, context: Context) -> ActionPlan:
        """
        Plan which tools to use and in what order.
        
        This phase determines:
        - Which tools are needed to fulfill the request
        - What parameters to pass to each tool
        - Whether tools should run sequentially or in parallel
        - Reasoning for the chosen plan
        
        Args:
            analysis: Result from analyze phase
            context: Processing context
            
        Returns:
            ActionPlan with tools, parameters, and strategy
            
        **Validates: Requirements 7.3**
        """
        logger.debug(f"Planning actions for intent: {analysis.intent}")
        
        # Get available tools
        available_tools = self.tool_registry.list_tools()
        tools_description = "\n".join([
            f"- {tool.name}: {tool.description}"
            for tool in available_tools
        ])
        
        planning_prompt = f"""Based on the user's intent, plan which tools to use.

Intent: {analysis.intent}
Required information: {', '.join(analysis.required_information)}
Entities: {json.dumps(analysis.entities)}

Available tools:
{tools_description}

Determine:
1. Which tools to use (in order)
2. Parameters for each tool
3. Execution strategy (sequential or parallel)
4. Reasoning for this plan

Respond in JSON format:
{{
  "tools_to_use": ["tool1", "tool2"],
  "tool_parameters": {{
    "tool1": {{"param": "value"}},
    "tool2": {{"param": "value"}}
  }},
  "execution_strategy": "sequential",
  "reasoning": "explanation of why this plan"
}}

Guidelines:
- Use profile_retrieval first if user data is needed
- Use opportunity_match for job/project searches
- Use web_search for external information
- Use parallel execution only if tools are independent
- If no tools needed, return empty tools_to_use array"""

        # Generate plan using LLM
        response_chunks = []
        async for chunk in self.llm_orchestrator.generate(
            prompt=planning_prompt,
            system_prompt=get_planning_prompt(),
            temperature=0.3,
            max_tokens=512
        ):
            response_chunks.append(chunk)
        
        response_text = "".join(response_chunks)
        
        # Parse JSON response
        plan_data = self._parse_json_response(response_text)
        
        return ActionPlan(
            tools_to_use=plan_data.get("tools_to_use", []),
            tool_parameters=plan_data.get("tool_parameters", {}),
            execution_strategy=plan_data.get("execution_strategy", "sequential"),
            reasoning=plan_data.get("reasoning", "No reasoning provided")
        )
    
    async def _act(self, plan: ActionPlan, context: Context) -> ActionResults:
        """
        Execute planned actions using tools.
        
        This phase executes the tool calls specified in the plan with:
        - Error handling for each tool
        - Timeout protection
        - Retry logic (handled by ToolRegistry)
        - Result validation
        - Parallel execution for independent tools
        
        Args:
            plan: Action plan from plan phase
            context: Processing context
            
        Returns:
            ActionResults with tool results and success status
            
        **Validates: Requirements 7.4, 10.5**
        """
        logger.debug(
            f"Executing {len(plan.tools_to_use)} tools with "
            f"{plan.execution_strategy} strategy"
        )
        
        tool_results: Dict[str, ToolResult] = {}
        errors: List[str] = []
        
        if not plan.tools_to_use:
            # No tools to execute
            logger.debug("No tools to execute")
            return ActionResults(
                tool_results={},
                success=True,
                errors=[]
            )
        
        # Execute tools based on strategy
        if plan.execution_strategy == "parallel":
            # Execute tools in parallel using ParallelExecutor
            logger.info(f"Executing {len(plan.tools_to_use)} tools in parallel")
            
            async def execute_tool(tool_name: str) -> tuple[str, ToolResult]:
                """Execute a single tool and return name with result."""
                parameters = plan.tool_parameters.get(tool_name, {})
                logger.debug(f"Executing tool: {tool_name} with params: {parameters}")
                
                result = await self.tool_registry.execute_tool(
                    tool_name=tool_name,
                    parameters=parameters,
                    timeout_seconds=30
                )
                return (tool_name, result)
            
            # Create callables for each tool
            tool_operations = [
                lambda tn=tool_name: execute_tool(tn)
                for tool_name in plan.tools_to_use
            ]
            
            # Execute in parallel with concurrency limit
            results = await ParallelExecutor.execute_parallel(
                operations=tool_operations,
                max_concurrent=5  # Limit concurrent tool executions
            )
            
            # Process results
            for result in results:
                if isinstance(result, Exception):
                    error_msg = f"Tool execution failed: {str(result)}"
                    errors.append(error_msg)
                    logger.error(error_msg)
                else:
                    tool_name, tool_result = result
                    tool_results[tool_name] = tool_result
                    
                    if not tool_result.success:
                        errors.append(
                            f"Tool {tool_name} failed: {tool_result.error}"
                        )
        
        else:
            # Execute tools sequentially
            logger.info(f"Executing {len(plan.tools_to_use)} tools sequentially")
            
            for tool_name in plan.tools_to_use:
                try:
                    parameters = plan.tool_parameters.get(tool_name, {})
                    
                    logger.debug(
                        f"Executing tool: {tool_name} with params: {parameters}"
                    )
                    
                    result = await self.tool_registry.execute_tool(
                        tool_name=tool_name,
                        parameters=parameters,
                        timeout_seconds=30
                    )
                    
                    tool_results[tool_name] = result
                    
                    if not result.success:
                        error_msg = f"Tool {tool_name} failed: {result.error}"
                        errors.append(error_msg)
                        logger.warning(error_msg)
                        
                        # Continue with other tools even if one fails
                        # This allows partial results
                
                except Exception as e:
                    error_msg = f"Tool {tool_name} execution error: {str(e)}"
                    errors.append(error_msg)
                    logger.error(error_msg, exc_info=True)
        
        # Determine overall success
        # Success if at least one tool succeeded or no tools were needed
        success = (
            len(tool_results) > 0 and
            any(result.success for result in tool_results.values())
        ) or len(plan.tools_to_use) == 0
        
        logger.info(
            f"Action execution complete: success={success}, "
            f"tools_executed={len(tool_results)}, errors={len(errors)}"
        )
        
        return ActionResults(
            tool_results=tool_results,
            success=success,
            errors=errors
        )
    
    async def _respond(
        self,
        analysis: Analysis,
        plan: ActionPlan,
        results: ActionResults,
        context: Context,
        stream: bool
    ) -> AsyncIterator[AgentResponse]:
        """
        Generate final response using LLM.
        
        This phase synthesizes all information into a natural language response:
        - Combines tool results with context
        - Generates user-friendly explanation
        - Provides actionable recommendations
        - Handles errors gracefully
        
        Args:
            analysis: Analysis from analyze phase
            plan: Plan from plan phase
            results: Results from act phase
            context: Processing context
            stream: Whether to stream response
            
        Yields:
            AgentResponse chunks if streaming, or complete response
            
        **Validates: Requirements 7.5**
        """
        logger.debug("Generating response")
        
        # Build response prompt with all context
        tool_results_summary = self._format_tool_results(results.tool_results)
        memory_context = self._format_memory_context(context.memory_entries)
        
        response_prompt = f"""Generate a helpful response to the user based on the following information.

User's message: "{context.message}"

Analysis:
- Intent: {analysis.intent}
- Confidence: {analysis.confidence:.2f}

Action plan:
- Tools used: {', '.join(plan.tools_to_use) if plan.tools_to_use else 'None'}
- Reasoning: {plan.reasoning}

Tool results:
{tool_results_summary}

Errors (if any):
{chr(10).join(f'- {error}' for error in results.errors) if results.errors else 'None'}

Recent conversation context:
{memory_context}

Generate a response that:
1. Directly addresses the user's request
2. Presents tool results in a clear, actionable way
3. Provides specific recommendations or next steps
4. Acknowledges any limitations or errors honestly
5. Maintains a professional but friendly tone
6. References previous conversation context when relevant

If tools failed or no results were found, explain why and suggest alternatives."""

        # Generate response using LLM
        response_content = []
        
        async for chunk in self.llm_orchestrator.generate(
            prompt=response_prompt,
            system_prompt=self.system_prompt,
            temperature=0.7,
            max_tokens=2048,
            stream=stream
        ):
            response_content.append(chunk)
            
            if stream:
                # Yield streaming chunk
                yield AgentResponse(
                    content=chunk,
                    conversation_id=context.conversation_id,
                    metadata={
                        "request_id": context.metadata.get("request_id"),
                        "streaming": True
                    },
                    timestamp=datetime.now()
                )
        
        # Build complete response
        full_response = "".join(response_content)
        
        # Build metadata
        metadata = {
            "request_id": context.metadata.get("request_id"),
            "intent": analysis.intent,
            "confidence": analysis.confidence,
            "tools_used": plan.tools_to_use,
            "tool_results": {
                name: {
                    "success": result.success,
                    "execution_time_ms": result.execution_time_ms
                }
                for name, result in results.tool_results.items()
            },
            "errors": results.errors,
            "streaming": False
        }
        
        # Store interaction in memory
        await self.memory_system.store_interaction(
            user_id=context.user_id,
            conversation_id=context.conversation_id,
            user_message=context.message,
            agent_response=full_response,
            metadata=metadata
        )
        
        if not stream:
            # Yield complete response
            yield AgentResponse(
                content=full_response,
                conversation_id=context.conversation_id,
                metadata=metadata,
                timestamp=datetime.now()
            )
    
    def _format_memory_context(self, memory_entries: List[Any]) -> str:
        """Format memory entries for prompt context."""
        if not memory_entries:
            return "No previous context"
        
        formatted = []
        for entry in memory_entries[:3]:  # Limit to 3 most relevant
            formatted.append(
                f"User: {entry.user_message}\n"
                f"Agent: {entry.agent_response}"
            )
        
        return "\n\n".join(formatted)
    
    def _format_tool_results(self, tool_results: Dict[str, ToolResult]) -> str:
        """Format tool results for prompt context."""
        if not tool_results:
            return "No tools were executed"
        
        formatted = []
        for tool_name, result in tool_results.items():
            if result.success:
                # Format successful result
                data_summary = str(result.data)[:500]  # Limit length
                formatted.append(
                    f"Tool: {tool_name}\n"
                    f"Status: Success\n"
                    f"Data: {data_summary}"
                )
            else:
                # Format error
                formatted.append(
                    f"Tool: {tool_name}\n"
                    f"Status: Failed\n"
                    f"Error: {result.error}"
                )
        
        return "\n\n".join(formatted)
    
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """Parse JSON from LLM response, handling markdown code blocks."""
        import re
        
        # Try to extract JSON from markdown code block
        json_match = re.search(
            r'```(?:json)?\s*(\{.*?\})\s*```',
            response_text,
            re.DOTALL
        )
        
        if json_match:
            json_text = json_match.group(1)
        else:
            # Try to find JSON object directly
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(0)
            else:
                logger.warning(f"Could not extract JSON from response: {response_text[:200]}")
                return {}
        
        try:
            return json.loads(json_text)
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            return {}
    
    def _generate_error_message(self, error: Exception) -> str:
        """Generate user-friendly error message."""
        error_type = type(error).__name__
        
        if "timeout" in str(error).lower():
            return (
                "I'm sorry, but my response is taking longer than expected. "
                "Please try again in a moment."
            )
        elif "rate limit" in str(error).lower():
            return (
                "I'm currently experiencing high demand. "
                "Please wait a moment and try again."
            )
        elif "authentication" in str(error).lower():
            return (
                "I'm having trouble connecting to my services. "
                "Please contact support if this persists."
            )
        else:
            return (
                "I encountered an unexpected issue while processing your request. "
                "Please try rephrasing your question or contact support if the problem continues."
            )
    
    def _is_simple_conversational_query(self, message: str) -> bool:
        """
        Determine if a query is simple conversational and doesn't need full ReAct.
        
        Simple queries include:
        - Greetings (hello, hi, hey)
        - General questions about the platform
        - Clarification requests
        - Simple factual questions
        
        Complex queries that need full ReAct:
        - Job search requests
        - Builder/collaborator matching
        - Document uploads
        - Interview preparation
        """
        message_lower = message.lower().strip()
        
        # Greetings and simple interactions
        simple_patterns = [
            'hello', 'hi', 'hey', 'good morning', 'good afternoon',
            'how are you', 'what can you do', 'help', 'what is',
            'tell me about', 'explain', 'who are you', 'what are you',
            'thanks', 'thank you', 'bye', 'goodbye'
        ]
        
        # Complex patterns that need tools
        complex_patterns = [
            'find job', 'search job', 'job opportunit', 'looking for work',
            'find builder', 'find collaborator', 'search developer',
            'upload', 'submit', 'resume', 'cv', 'portfolio',
            'interview prep', 'practice interview', 'mock interview',
            'match me', 'recommend', 'suggest project'
        ]
        
        # Check for complex patterns first
        for pattern in complex_patterns:
            if pattern in message_lower:
                return False
        
        # Check for simple patterns
        for pattern in simple_patterns:
            if pattern in message_lower:
                return True
        
        # Short messages are usually simple
        if len(message.split()) <= 5:
            return True
        
        # Default to simple for safety (can be adjusted based on usage)
        return True
    
    async def _respond_simple(
        self,
        message: str,
        context: Context,
        stream: bool
    ) -> AsyncIterator[AgentResponse]:
        """
        Generate a simple response without full ReAct cycle.
        
        This is used for conversational queries that don't require
        tool usage or complex reasoning.
        """
        logger.debug("Generating simple response (fast path)")
        
        # Build simple prompt
        memory_context = self._format_memory_context(context.memory_entries)
        
        simple_prompt = f"""You are Magna AI, a career assistant for the Magna platform.

User's message: "{message}"

Previous context:
{memory_context if memory_context else "No previous context"}

Provide a helpful, friendly response. Keep it concise and conversational.
If the user is asking about what you can do, mention:
- Finding job opportunities
- Searching for builders and collaborators
- Preparing for interviews
- Managing career documents

Response:"""
        
        # Generate response
        response_text = ""
        async for chunk in self.llm_orchestrator.generate(
            prompt=simple_prompt,
            system_prompt=BASE_SYSTEM_PROMPT,
            stream=stream
        ):
            response_text += chunk
            
            # Yield chunk if streaming
            if stream:
                yield AgentResponse(
                    content=chunk,
                    conversation_id=context.conversation_id,
                    metadata={
                        "request_id": context.metadata.get("request_id"),
                        "fast_path": True
                    },
                    timestamp=datetime.now()
                )
        
        # Yield complete response if not streaming
        if not stream:
            yield AgentResponse(
                content=response_text,
                conversation_id=context.conversation_id,
                metadata={
                    "request_id": context.metadata.get("request_id"),
                    "fast_path": True
                },
                timestamp=datetime.now()
            )
