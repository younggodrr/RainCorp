# ============================================
# Magna AI Backend Environment Configuration
# ============================================

# Main Backend Integration (REQUIRED)
# URL of the Magna Coders main backend API
BACKEND_API_URL="http://localhost:5000"

# Secure API key for authenticating with main backend (min 32 characters)
# IMPORTANT: Change this in production! Must match AI_API_KEY in main backend .env
BACKEND_API_KEY="your-secure-api-key-at-least-32-characters-long-change-in-production"

# Database (REQUIRED)
# PostgreSQL connection string (shared with main backend)
DATABASE_URL="postgresql://postgres:password@localhost:5432/magna_coders?schema=public"

# Security (REQUIRED)
# Secret key for JWT token validation (must match main backend JWT_SECRET)
JWT_SECRET="your-super-secret-jwt-key-change-this-in-production"

# Encryption key for sensitive data (exactly 32 characters)
ENCRYPTION_KEY="change-this-32-char-encryption"

# LLM Providers (REQUIRED - at least one)
# Google Gemini API key (recommended for production)
GEMINI_API_KEY="your-gemini-api-key-here"

# OpenAI API key (optional)
OPENAI_API_KEY=""

# NVIDIA NIM API key and model (optional)
NVIDIA_NIM_API_KEY=""
NVIDIA_NIM_MODEL="meta/llama-3.1-8b-instruct"

# Ollama base URL for local models (optional)
OLLAMA_BASE_URL="http://localhost:11434"

# Search (Optional)
# SerpAPI key for web search capabilities
SERPAPI_API_KEY=""

# AWS S3 (Optional)
# For document storage and retrieval
AWS_ACCESS_KEY_ID=""
AWS_SECRET_ACCESS_KEY=""
AWS_REGION="us-east-1"
S3_BUCKET_NAME="magna-ai-documents"

# Vector Database - Pinecone (Optional)
# For semantic search and profile matching
PINECONE_API_KEY=""
PINECONE_ENVIRONMENT="us-east-1-aws"
PINECONE_INDEX_NAME="magna-ai-profiles"

# API Configuration
# Host and port for the FastAPI server
API_HOST="0.0.0.0"
API_PORT=8000
API_WORKERS=4

# CORS origins (comma-separated list)
# IMPORTANT: Update for production with your frontend domain
CORS_ORIGINS="http://localhost:3000"

# Environment: development or production
ENVIRONMENT="development"

# Rate Limiting
# Requests per minute per user (tier-based limits applied in code)
RATE_LIMIT_PER_MINUTE=60

# Memory Configuration
# Maximum memory size in MB before pruning
MAX_MEMORY_SIZE_MB=5
MEMORY_PRUNE_THRESHOLD_MB=4.5

# LLM Configuration
# Default parameters for LLM generation
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
LLM_MAX_TOKENS=2048
LLM_TIMEOUT_SECONDS=30

# Logging
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL="INFO"

# Log format: json or text
LOG_FORMAT="json"

# Feature Flags
# Enable local model support (Ollama)
ENABLE_LOCAL_MODELS=false

# Enable memory synchronization across sessions
ENABLE_MEMORY_SYNC=true

# Enable analytics and usage tracking
ENABLE_ANALYTICS=true

# ============================================
# Production Deployment Notes
# ============================================
# 
# 1. Change all "change-this" and "your-*" placeholder values
# 2. Use strong, randomly generated keys (min 32 characters)
# 3. Enable HTTPS/TLS for inter-service communication
# 4. Update CORS_ORIGINS to match your production frontend domain
# 5. Set ENVIRONMENT="production"
# 6. Use secure secret management (AWS Secrets Manager, HashiCorp Vault, etc.)
# 7. Ensure BACKEND_API_KEY matches AI_API_KEY in main backend
# 8. Ensure JWT_SECRET matches main backend JWT_SECRET
# 9. Use production-grade database with connection pooling
# 10. Configure proper logging and monitoring
